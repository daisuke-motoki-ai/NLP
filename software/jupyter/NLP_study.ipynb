{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分かち書き\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人間\t名詞,一般,*,*,*,*,人間,ニンゲン,ニンゲン\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "日常\t名詞,一般,*,*,*,*,日常,ニチジョウ,ニチジョー\n",
      "的\t名詞,接尾,形容動詞語幹,*,*,*,的,テキ,テキ\n",
      "に\t助詞,副詞化,*,*,*,*,に,ニ,ニ\n",
      "使っ\t動詞,自立,*,*,五段・ワ行促音便,連用タ接続,使う,ツカッ,ツカッ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いる\t動詞,非自立,*,*,一段,基本形,いる,イル,イル\n",
      "自然\t名詞,形容動詞語幹,*,*,*,*,自然,シゼン,シゼン\n",
      "言語\t名詞,一般,*,*,*,*,言語,ゲンゴ,ゲンゴ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "コンピュータ\t名詞,一般,*,*,*,*,コンピュータ,コンピュータ,コンピュータ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "処理\t名詞,サ変接続,*,*,*,*,処理,ショリ,ショリ\n",
      "さ\t動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ\n",
      "せる\t動詞,接尾,*,*,一段,基本形,せる,セル,セル\n",
      "一連\t名詞,一般,*,*,*,*,一連,イチレン,イチレン\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "技術\t名詞,一般,*,*,*,*,技術,ギジュツ,ギジュツ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "あり\t助動詞,*,*,*,五段・ラ行アル,連用形,ある,アリ,アリ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "人工\t名詞,一般,*,*,*,*,人工,ジンコウ,ジンコー\n",
      "知能\t名詞,一般,*,*,*,*,知能,チノウ,チノー\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "言語\t名詞,一般,*,*,*,*,言語,ゲンゴ,ゲンゴ\n",
      "学\t名詞,接尾,一般,*,*,*,学,ガク,ガク\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
      "分野\t名詞,一般,*,*,*,*,分野,ブンヤ,ブンヤ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\n",
      "言語\t名詞,一般,*,*,*,*,言語,ゲンゴ,ゲンゴ\n",
      "学\t名詞,接尾,一般,*,*,*,学,ガク,ガク\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tagger.parse('人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の一分野である。「計算言語学」'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人間',\n",
       " 'が',\n",
       " '日常',\n",
       " '的',\n",
       " 'に',\n",
       " '使っ',\n",
       " 'て',\n",
       " 'いる',\n",
       " '自然',\n",
       " '言語',\n",
       " 'を',\n",
       " 'コンピュータ',\n",
       " 'に',\n",
       " '処理',\n",
       " 'さ',\n",
       " 'せる',\n",
       " '一連',\n",
       " 'の',\n",
       " '技術',\n",
       " 'で',\n",
       " 'あり',\n",
       " '、',\n",
       " '人工',\n",
       " '知能',\n",
       " 'と',\n",
       " '言語',\n",
       " '学',\n",
       " 'の',\n",
       " '一',\n",
       " '分野',\n",
       " 'で',\n",
       " 'ある',\n",
       " '。',\n",
       " '「',\n",
       " '計算',\n",
       " '言語',\n",
       " '学',\n",
       " '」']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の一分野である。「計算言語学」')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人間 が 日常 的 に 使っ て いる 自然 言語 を コンピュータ に 処理 さ せる 一連 の 技術 で あり 、 人工 知能 と 言語 学 の 一 分野 で ある 。 「 計算 言語 学 」 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tagger.parse('人間が日常的に使っている自然言語をコンピュータに処理させる一連の技術であり、人工知能と言語学の一分野である。「計算言語学」'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words（Bow）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizer\n",
      "  Downloading tokenizer-2.0.4-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizer\n",
      "Successfully installed tokenizer-2.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 0, 0], [1, 0, 1, 1, 0], [1, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from tokenizer import tokenize\n",
    "\n",
    "\n",
    "def calc_bow(tokenized_texts):\n",
    "    counts = [Counter(tokenized_text)\n",
    "              for tokenized_text in tokenized_texts]  # <1>\n",
    "    sum_counts = sum(counts, Counter())  # <2>\n",
    "    vocabulary = sum_counts.keys()\n",
    "\n",
    "    bow = [[count[word] for word in vocabulary]\n",
    "           for count in counts]  # <3>\n",
    "\n",
    "    return bow\n",
    "\n",
    "\n",
    "# 入力文のlist\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "bow = calc_bow(tokenized_texts)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize  # <1>\n",
    "\n",
    "\n",
    "def calc_bow(tokenized_texts):  # <2>\n",
    "    # Build vocabulary <3>\n",
    "    vocabulary = {}\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        for token in tokenized_text:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = len(vocabulary)\n",
    "\n",
    "    n_vocab = len(vocabulary)\n",
    "\n",
    "    # Build BoW Feature Vector <4>\n",
    "    bow = [[0] * n_vocab for i in range(len(tokenized_texts))]\n",
    "    for i, tokenized_text in enumerate(tokenized_texts):\n",
    "        for token in tokenized_text:\n",
    "            index = vocabulary[token]\n",
    "            bow[i][index] += 1\n",
    "\n",
    "    return vocabulary, bow\n",
    "\n",
    "\n",
    "# 入力文のlist\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "vocabulary, bow = calc_bow(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tokenizer import tokenize  # <1>\n",
    "\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです。',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "# Bag of Words計算\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)  # <2>\n",
    "vectorizer.fit(texts)  # <3>\n",
    "bow = vectorizer.transform(texts)  # <4>\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 識別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c343bb873d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mBASE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <3>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, join, normpath\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class DialogueAgent:\n",
    "    def __init__(self):\n",
    "        self.tagger = MeCab.Tagger()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        node = self.tagger.parseToNode(text)\n",
    "\n",
    "        tokens = []\n",
    "        while node:\n",
    "            if node.surface != '':\n",
    "                tokens.append(node.surface)\n",
    "\n",
    "            node = node.next\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def train(self, texts, labels):\n",
    "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
    "        bow = vectorizer.fit_transform(texts)  # <1>\n",
    "\n",
    "        classifier = SVC()\n",
    "        classifier.fit(bow, labels)\n",
    "\n",
    "        # <2>\n",
    "        self.vectorizer = vectorizer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def predict(self, texts):\n",
    "        bow = self.vectorizer.transform(texts)\n",
    "        return self.classifier.predict(bow)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BASE_DIR = normpath(dirname(__file__))\n",
    "\n",
    "    training_data = pd.read_csv(join(BASE_DIR, './training_data.csv'))  # <3>\n",
    "\n",
    "    dialogue_agent = DialogueAgent()\n",
    "    dialogue_agent.train(training_data['text'], training_data['label'])\n",
    "\n",
    "    with open(join(BASE_DIR, './replies.csv')) as f:  # <4>\n",
    "        replies = f.read().split('\\n')\n",
    "\n",
    "    input_text = '名前を教えてよ'\n",
    "    predictions = dialogue_agent.predict([input_text])  # <5>\n",
    "    predicted_class_id = predictions[0]  # <6>\n",
    "\n",
    "    print(replies[predicted_class_id])\n",
    "\n",
    "    while True:\n",
    "        input_text = input()\n",
    "        predictions = dialogue_agent.predict([input_text])\n",
    "        predicted_class_id = predictions[0]\n",
    "\n",
    "        print(replies[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"src/sec30_lets_create_a_dialogue_agent/sampleapp/dialogue_agent.py\", line 44, in <module>\r\n",
      "    training_data = pd.read_csv(join(BASE_DIR, './training_data.csv'))  # <3>\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 676, in parser_f\r\n",
      "    return _read(filepath_or_buffer, kwds)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 448, in _read\r\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 880, in __init__\r\n",
      "    self._make_engine(self.engine)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1114, in _make_engine\r\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1891, in __init__\r\n",
      "    self._reader = parsers.TextReader(src, **kwds)\r\n",
      "  File \"pandas/_libs/parsers.pyx\", line 374, in pandas._libs.parsers.TextReader.__cinit__\r\n",
      "  File \"pandas/_libs/parsers.pyx\", line 673, in pandas._libs.parsers.TextReader._setup_parser_source\r\n",
      "FileNotFoundError: [Errno 2] File src/sec30_lets_create_a_dialogue_agent/sampleapp/./training_data.csv does not exist: 'src/sec30_lets_create_a_dialogue_agent/sampleapp/./training_data.csv'\r\n"
     ]
    }
   ],
   "source": [
    "!python src/sec30_lets_create_a_dialogue_agent/sampleapp/dialogue_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
