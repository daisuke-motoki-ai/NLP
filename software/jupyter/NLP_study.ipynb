{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分かち書き\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger()\n",
    "mytext = 'こんにちは元木です元気ですか'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは\t感動詞,*,*,*,*,*,こんにちは,コンニチハ,コンニチワ\n",
      "元木\t名詞,固有名詞,人名,姓,*,*,元木,モトキ,モトキ\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "元気\t名詞,形容動詞語幹,*,*,*,*,元気,ゲンキ,ゲンキ\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tagger.parse(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            tokens.append(node.surface)\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['こんにちは', '元木', 'です', '元気', 'です', 'か']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは 元木 です 元気 です か \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tagger.parse(mytext\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words（Bow）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    mytext\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです',\n",
    "    '富士山は日本一高い山です',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tokenizer import tokenize\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = tokenize)\n",
    "vectorizer.fit(texts)\n",
    "bow = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>あなた</th>\n",
       "      <th>が</th>\n",
       "      <th>こと</th>\n",
       "      <th>です</th>\n",
       "      <th>な</th>\n",
       "      <th>の</th>\n",
       "      <th>は</th>\n",
       "      <th>ラーメン</th>\n",
       "      <th>好き</th>\n",
       "      <th>富士山</th>\n",
       "      <th>山</th>\n",
       "      <th>日本一</th>\n",
       "      <th>私</th>\n",
       "      <th>高い</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   あなた  が  こと  です  な  の  は  ラーメン  好き  富士山  山  日本一  私  高い\n",
       "0    1  2   1   1  1  1  1     0   2    0  0    0  2   0\n",
       "1    0  1   0   1  0  0  1     1   1    0  0    0  1   0\n",
       "2    0  0   0   1  0  0  1     0   0    1  1    1  0   1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bow_table = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names())\n",
    "print('Shape: {}'.format(bow.shape))\n",
    "bow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizer\n",
      "  Downloading tokenizer-2.0.4-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizer\n",
      "Successfully installed tokenizer-2.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-909d4bf03a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtokenized_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from tokenizer import tokenize\n",
    "\n",
    "\n",
    "def calc_bow(tokenized_texts):\n",
    "    counts = [Counter(tokenized_text)\n",
    "              for tokenized_text in tokenized_texts]  # <1>\n",
    "    sum_counts = sum(counts, Counter())  # <2>\n",
    "    vocabulary = sum_counts.keys()\n",
    "\n",
    "    bow = [[count[word] for word in vocabulary]\n",
    "           for count in counts]  # <3>\n",
    "\n",
    "    return vocabulary, bow\n",
    "\n",
    "\n",
    "# 入力文のlist\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "bow = calc_bow(tokenized_texts)\n",
    "vocabulary, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([Tok(kind=11001, txt=None, val=(0, None)), Tok(kind=6, txt='こんにちは元木です元気ですか', val=None), Tok(kind=11002, txt=None, val=None)]),\n",
       " [[1, 1, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力文のlist\n",
    "texts = [\n",
    "    mytext\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "bow = calc_bow(tokenized_texts)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize  # <1>\n",
    "\n",
    "\n",
    "def calc_bow(tokenized_texts):  # <2>\n",
    "    # Build vocabulary <3>\n",
    "    vocabulary = {}\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        for token in tokenized_text:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = len(vocabulary)\n",
    "\n",
    "    n_vocab = len(vocabulary)\n",
    "\n",
    "    # Build BoW Feature Vector <4>\n",
    "    bow = [[0] * n_vocab for i in range(len(tokenized_texts))]\n",
    "    for i, tokenized_text in enumerate(tokenized_texts):\n",
    "        for token in tokenized_text:\n",
    "            index = vocabulary[token]\n",
    "            bow[i][index] += 1\n",
    "\n",
    "    return vocabulary, bow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 入力文のlist\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "vocabulary, bow = calc_bow(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'私': 0, 'は': 1, 'の': 2, 'こと': 3, 'が': 4, '好き': 5, 'な': 6, 'あなた': 7, 'です': 8, 'ラーメン': 9, '富士山': 10, '日本一': 11, '高い': 12, '山': 13} [[2, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary, bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'こんにちは': 0, '元木': 1, 'です': 2, '元気': 3, 'か': 4}, [[1, 1, 2, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力文のlist\n",
    "texts = [\n",
    "    mytext\n",
    "]\n",
    "\n",
    "tokenized_texts = [tokenize(text) for text in texts]\n",
    "vocabulary, bow = calc_bow(tokenized_texts)\n",
    "vocabulary, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'こんにちは': 0, '元木': 1, 'です': 2, '元気': 3, 'か': 4}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word\n",
       "0     1\n",
       "1     2\n",
       "2     3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = [1, 2, 3]\n",
    "df = pd.DataFrame({'word':aaa})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>か</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>こんにちは</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>です</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>元木</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>元気</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word\n",
       "か         4\n",
       "こんにちは     0\n",
       "です        2\n",
       "元木        1\n",
       "元気        3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_df = pd.DataFrame({'word':vocabulary})\n",
    "all_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t2\n",
      "  (0, 13)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 14)\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tokenizer import tokenize  # <1>\n",
    "\n",
    "texts = [\n",
    "    '私は私のことが好きなあなたが好きです',\n",
    "    '私はラーメンが好きです。',\n",
    "    '富士山は日本一高い山です',\n",
    "]\n",
    "\n",
    "# Bag of Words計算\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)  # <2>\n",
    "vectorizer.fit(texts)  # <3>\n",
    "bow = vectorizer.transform(texts)  # <4>\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 識別器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c343bb873d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mBASE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./training_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <3>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, join, normpath\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class DialogueAgent:\n",
    "    def __init__(self):\n",
    "        self.tagger = MeCab.Tagger()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        node = self.tagger.parseToNode(text)\n",
    "\n",
    "        tokens = []\n",
    "        while node:\n",
    "            if node.surface != '':\n",
    "                tokens.append(node.surface)\n",
    "\n",
    "            node = node.next\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def train(self, texts, labels):\n",
    "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
    "        bow = vectorizer.fit_transform(texts)  # <1>\n",
    "\n",
    "        classifier = SVC()\n",
    "        classifier.fit(bow, labels)\n",
    "\n",
    "        # <2>\n",
    "        self.vectorizer = vectorizer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def predict(self, texts):\n",
    "        bow = self.vectorizer.transform(texts)\n",
    "        return self.classifier.predict(bow)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BASE_DIR = normpath(dirname(__file__))\n",
    "\n",
    "    training_data = pd.read_csv(join(BASE_DIR, './training_data.csv'))  # <3>\n",
    "\n",
    "    dialogue_agent = DialogueAgent()\n",
    "    dialogue_agent.train(training_data['text'], training_data['label'])\n",
    "\n",
    "    with open(join(BASE_DIR, './replies.csv')) as f:  # <4>\n",
    "        replies = f.read().split('\\n')\n",
    "\n",
    "    input_text = '名前を教えてよ'\n",
    "    predictions = dialogue_agent.predict([input_text])  # <5>\n",
    "    predicted_class_id = predictions[0]  # <6>\n",
    "\n",
    "    print(replies[predicted_class_id])\n",
    "\n",
    "    while True:\n",
    "        input_text = input()\n",
    "        predictions = dialogue_agent.predict([input_text])\n",
    "        predicted_class_id = predictions[0]\n",
    "\n",
    "        print(replies[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'src/sec30_lets_create_a_dialogue_agent/sampleapp/dialogue_agent.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python src/sec30_lets_create_a_dialogue_agent/sampleapp/dialogue_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
